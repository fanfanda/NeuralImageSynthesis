{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "require 'torch'\n",
    "require 'nn'\n",
    "require 'optim'\n",
    "require 'loadcaffe'\n",
    "require 'hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {gpu = 0,backend = 'cudnn', \n",
    "    caffe_model = '/home/fanfanda/style_transfer/fanfanda_neuralImages/NeuralImageSynthesis/Models/VGG_ILSVRC_19_layers_conv.caffemodel', \n",
    "    input_file = '/home/fanfanda/style_transfer/fanfanda_neuralImages/NeuralImageSynthesis/Tmp/input.hdf5', \n",
    "    init_file = '/home/fanfanda/style_transfer/fanfanda_neuralImages/NeuralImageSynthesis/Tmp/init.hdf5',\n",
    "    max_iter = 50, print_iter = 50, save_iter = 0, layer_order = 'relu1_1,relu2_1,relu3_1,relu4_1,relu5_1,relu4_2',\n",
    "    output_file = '/home/fanfanda/style_transfer/fanfanda_neuralImages/NeuralImageSynthesis/Tmp/output.hdf', mask_file = 'path/to/HDF5file'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths.dofile('LossLayers.lua')\n",
    "paths.dofile('Misc.lua')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Set gpu mode\n",
    "if params.gpu >= 0 then\n",
    "    require 'cutorch'\n",
    "    require 'cunn'\n",
    "    cutorch.setDevice(params.gpu + 1)\n",
    "else\n",
    "    params.backend = 'nn'\n",
    "end\n",
    "if params.backend == 'cudnn' then\n",
    "    require 'cudnn'\n",
    "    if params.cudnn_autotune then\n",
    "        cudnn.benchmark = true\n",
    "    end\n",
    "    cudnn.SpatialConvolution.accGradParameters = nn.SpatialConvolutionMM.accGradParameters -- ie: nop\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Successfully loaded /home/fanfanda/style_transfer/fanfanda_neuralImages/NeuralImageSynthesis/Models/VGG_ILSVRC_19_layers_conv.caffemodel\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv1_1: 64 3 3 3\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv1_2: 64 64 3 3\n",
       "conv2_1: 128 64 3 3\n",
       "conv2_2: 128 128 3 3\n",
       "conv3_1: 256 128 3 3\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv3_2: 256 256 3 3\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv3_3: 256 256 3 3\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv3_4: 256 256 3 3\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv4_1: 512 256 3 3\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv4_2: 512 512 3 3\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv4_3: 512 512 3 3\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv4_4: 512 512 3 3\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv5_1: 512 512 3 3\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv5_2: 512 512 3 3\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv5_3: 512 512 3 3\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "conv5_4: 512 512 3 3\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- Load network from caffemodel\n",
    "local loadcaffe_backend = params.backend\n",
    "cnn = loadcaffe.load('network', params.caffe_model, params.backend):float()\n",
    "cnn = set_datatype(cnn, params.gpu)\n",
    "\n",
    "-- Load optimisation targets \n",
    "local f = hdf5.open(params.input_file, 'r')\n",
    "opt_targets = f:all()\n",
    "f:close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Set up new network with appropriate loss layers\n",
    "net = nn.Sequential()\n",
    "loss_modules = {}\n",
    "next_layer_ndx = 1\n",
    "-- Loss layers acting directly on the image\n",
    "if opt_targets['data'] then\n",
    "    loss_modules['data'] = {}\n",
    "    for loss_layer, args in pairs(opt_targets['data']) do\n",
    "        local loss_module = get_loss_module(loss_layer, args)\n",
    "        loss_module = set_datatype(loss_module, params.gpu)\n",
    "        net:add(loss_module)\n",
    "        loss_modules['data'][loss_layer] = loss_module\n",
    "    end\n",
    "    next_layer_ndx = next_layer_ndx + 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Loss layers acting on CNN features\n",
    "for i = 1, #cnn do\n",
    "    if next_layer_ndx <= length(opt_targets) then\n",
    "        local layer = cnn:get(i)\n",
    "        local layer_name = layer.name\n",
    "        local layer_type = torch.type(layer)\n",
    "        local is_convolution = (layer_type == 'cudnn.SpatialConvolution' or layer_type == 'nn.SpatialConvolution')\n",
    "        if is_convolution and params.reflectance then\n",
    "            local padW, padH = layer.padW, layer.padH\n",
    "            local pad_layer = nn.SpatialReflectionPadding(padW, padW, padH, padH)\n",
    "            pad_layer = set_datatype(pad_layer, params.gpu)\n",
    "            net:add(pad_layer)\n",
    "            layer.padW = 0\n",
    "            layer.padH = 0\n",
    "        end\n",
    "        net:add(layer)\n",
    "        if opt_targets[layer_name] then\n",
    "            loss_modules[layer_name] = {}\n",
    "            for loss_layer, args in pairs(opt_targets[layer_name]) do\n",
    "                if loss_layer == 'GramMSEDilation' then\n",
    "                    args['conv_layer'] = net.modules[#net.modules-1]\n",
    "                    local dilation_losses = get_loss_module(loss_layer, args)\n",
    "                    for i, dl in ipairs(dilation_losses) do \n",
    "                        dl = set_datatype(dl, params.gpu)\n",
    "                        table.insert(net.modules, #net.modules-1, dl)\n",
    "                    end\n",
    "                    loss_modules[layer_name][loss_layer] = dilation_losses\n",
    "                else\n",
    "                    local loss_module = get_loss_module(loss_layer, args)\n",
    "                    loss_module = set_datatype(loss_module, params.gpu)\n",
    "                    net:add(loss_module)\n",
    "                    loss_modules[layer_name][loss_layer] = loss_module\n",
    "                end\n",
    "            end\n",
    "            next_layer_ndx = next_layer_ndx + 1\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Get flat list of loss modules to call in feval\n",
    "loss_modules_flat = {}\n",
    "for layer_name, layer_table in pairs(loss_modules) do\n",
    "    for loss_layer, loss_module in pairs(layer_table) do\n",
    "        if loss_layer == 'GramMSEDilation' then\n",
    "            for _, dilation_module in pairs(loss_module) do\n",
    "                loss_modules_flat[#loss_modules_flat + 1] = dilation_module\n",
    "            end\n",
    "        else\n",
    "            loss_modules_flat[#loss_modules_flat + 1] = loss_module\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- We don't need the base CNN anymore, so clean it up to save memory.\n",
    "cnn = nil\n",
    "for i=1, #net.modules do\n",
    "    local module = net.modules[i]\n",
    "    if torch.type(module) == 'nn.SpatialConvolutionMM' then\n",
    "        module.gradWeight = nil\n",
    "        module.gradBias = nil\n",
    "    end\n",
    "end\n",
    "collectgarbage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Load initialisation \n",
    "local f = hdf5.open(params.init_file, 'r')\n",
    "img = f:all()['init']\n",
    "f:close()\n",
    "img = set_datatype(img, params.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Load mask if specified\n",
    "mask = nil\n",
    "if params.mask_file ~= 'path/to/HDF5file' then\n",
    "    local f = hdf5.open(params.mask_file, 'r')\n",
    "    mask = f:all()['mask']\n",
    "    f:close()\n",
    "    mask = set_datatype(mask, params.gpu)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Run it through the network once to get the proper size for the gradient\n",
    "-- All the gradients will come from the extra loss modules, so we just pass\n",
    "-- zeros into the top of the net on the backward pass.\n",
    "y = net:forward(img)\n",
    "dy = img.new(#y):zero()\n",
    "\n",
    "-- Declare optimisation options\n",
    "optim_state = {\n",
    "  maxIter = params.max_iter,\n",
    "  verbose = true,\n",
    "  tolX = 0,\n",
    "  tolFun = 0,\n",
    "}\n",
    "\n",
    "-- Get layer_order for use in maybe_print\n",
    "layer_order = params.layer_order:split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Function to evaluate loss and gradient. We run the net forward and\n",
    "-- backward to get the gradient, and sum up losses from the loss modules.\n",
    "-- optim.lbfgs internally handles iteration and calls this fucntion many\n",
    "-- times, so we manually count the number of iterations to handle printing\n",
    "-- and saving intermediate results.\n",
    "num_calls = 0\n",
    "local function feval(x)\n",
    "    num_calls = num_calls + 1\n",
    "    net:forward(x)\n",
    "    local grad = net:updateGradInput(x, dy)\n",
    "    local loss = 0\n",
    "    for _, mod in ipairs(loss_modules_flat) do\n",
    "        loss = loss + mod.loss\n",
    "    end\n",
    "    maybe_print(num_calls, params.print_iter, params.max_iter, layer_order, loss_modules, loss)\n",
    "    maybe_save(num_calls, params.save_iter, params.max_iter, params.output_file, img)\n",
    "\n",
    "    if mask then\n",
    "        grad[mask:repeatTensor(1,1,1):expandAs(grad)] = 0\n",
    "    end\n",
    "\n",
    "    collectgarbage()\n",
    "    -- optim.lbfgs expects a vector for gradients\n",
    "    return loss, grad:view(grad:nElement())\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
